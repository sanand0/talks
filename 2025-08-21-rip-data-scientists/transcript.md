# Transcript - RIP Data Scientists

Every single person has asked me the same question. Seriously? Are you kidding? RIP data scientists? Do you really believe that? Is that going to happen? Are we out of a job? Are you out of a job?

I'll tell you where it happened. A few months ago, I was at a workshop with a client organized by a partner. And they were trying to model floor tiles. They had enormous data where for every single type of floor tile they used, they knew how much it cost, how much load it could bear, and what was the carbon dioxide emission. The three parameters they wanted to optimize for. So if somebody said I want this room to be tiled, then they had the data for every kind of tile to show the performance on these parameters. Their objective was, can we build a model that, given the same inputs, will tell us which tile to use, optimizing for the three parameters.

They had a pair of data scientists, moderately experienced ones, working on this for two weeks, making slow, steady progress, but not fast enough. So the business lead on this looped us in and said, can you solve this problem? And took about half an hour to explain the problem, which I recorded, fed into ChatGPT, had it take the data. It wasn't massive, it was about 50 MB. Had it build the models, multiple models, figure out which model had the highest quality, generate the pickle files, download it, give me the Streamlit or HTTP or Flask API to serve the model and give it to them. 15 minutes. Done.

At which point it struck me, yeah, something has changed.

What does a data scientist do? Let's say your manager gives you a data set. Now, this is not an atypical scenario. The manager says, let's say we have a new client, Naya Airline. And what they want to do is launch a new airline in India, as the name suggests. And their operation head wants to know, where should I start my initial routes? Which cities will make me money, which routes will always be full, and what should I be pricing it at? That sort of a thing.

Now the data scientist doesn't know about this, doesn't care about it. What do you want me to do? But this is not atypical. This is the kind of brief that you get. And the data scientist will then go on to do what every data scientist does. Explore the data. And that will form an output. EDA, I have spent my first two weeks being productive and here is a 25-page, sometimes 250-page deck with all kinds of useless analysis that nobody's going to read, but I am productive. Congratulations.

Clean it, which we totally hate. So we'll generally spend a week against the two weeks of it doing something, who cares. And just let the client know your data is unclean in these 30 ways. Later on when my analysis is bad, these are the things that I will blame you for, but I will say that I've cleaned it. Model it, which I like. That's fun. We'll do more of that. We'll optimize it. We'll learn new techniques. Good fun. And then we are called to explain the model in simple terms. So we will go there and say, I did this, then I did this, then I read about this, and now there is a new technique that you know nothing about. And the audience will say, can you simplify it? Can you give it to me in business language? Idiots. Ignore them.

Deploy it. There'll be some data engineer, I'll toss the model over to them, let them deploy it. Not my problem. But if you need some support, contact me. I'm too cool for these sorts of things. And then the project manager will say, now we want new clients to be able to see this as a case study, so anonymize the data. Do something. Classic lifecycle. What does it take to automate this whole cycle?

Let's start by exploring the data, but in a hypothesis-driven way. That is, realistically, we don't know what the business wants. Arguably, the business doesn't know what they want. That's a different problem. But I know even less than they do. But LLMs are supposed to be so smart. Could we do something? So here's a little app that we can try prototyping. I call it Hypothesis Forge. What this does is allows me to upload a data set. I'm going to upload the daily airline traffic data from the DGCA, Director General, I have no idea what it stands for. But it's basically airline data in India and has information like which, what are the domestic airport aircraft movements, footfalls, lots of columns.

Normally I would do EDA on this, right? So I'd spend a lot of time trying to understand this data. And really the purpose of EDA is not to tell somebody else what the data is about, it's to learn what the data is about. But with LLMs, I don't really care. I'm going to say, "Naya Airlines wants to launch in India. How can they make more money? Think like a businessman. Actually, think like a greedy businessman." `[LAUGHTER]`

And generate the hypotheses. Now, this works pretty well, partly because we've told it how to think. But partly also because we've given it the data, not the entire data. It's a reasonably large data set. We're just taking the column headers, a few sample rows, doing some amount of statistics around it, saying, you know, here's the range of values, here are the typical columns, etc. and let it generate the hypotheses. Now, here is the thing that we are learning about LLMs. **They aren't better than the best that we have. But they are better than the worst or even the average that we have in many areas.** So I can now suddenly be at least an average MBA with a computer science degree, or even without a computer science degree. And these hypotheses are not bad. For instance, if I, it says, "Launch a predominantly domestic point-to-point network targeting the busiest metros and high footfall airports first." And it's saying using this data. Okay, yeah, that kind of makes sense. So if we did that, then does that mean that we will, you know, is there a statistically significant improvement in traffic because of that?

Now, it's generated the hypotheses. I need to test it. I am lazy. So I would rather have it write the code. One of the beauties of LLMs is that **they suck at mathematics. They can't even multiply eight-digit numbers as well as my pocket calculator. But they are good at language. Code is a language.** They're getting better and better at code. So let it write the code. Why should I write the code? And once it does that, if it is doing it, let me just check if there is an error. No, there isn't. Okay. It's just taking its own sweet time generating the code, which it hopefully eventually will.

The other thing that we are realizing is that we can also have it run the code. Now that this code is being generated, it will, and it seems to be doing a reasonably good job, meaning, let's see, it's figuring out some concatenations. Yeah, okay. And it's doing a correlation, a Pearson R correlation. It's also checking if the values are statistically significant. This is what I would do as a data scientist anyway. Just check if the results are statistically significant, if it's going in a certain direction. Ultimately, all I do is the business analyst tells me, is this true? I write a program to test it.

It's run the program and it's saying, hmm, this is green. Data does not, okay, does not support launching a domestic point-to-point network focused only on the top 8 to 12 airports. So it rejects the hypothesis because it does not account for the observed traffic patterns. There is overwhelming evidence that blah, blah, blah, and it's not due to chance, but basically if you just focus on these, you're not going to make money. Okay, that is useful to know. In about a minute, we have tested something and gotten a result. But I'm lazy. I don't like having to sit and read all of these. I am instead just going to say test them all. `[LAUGHTER]` Let it run in parallel. `[LAUGHTER]`

**Rigor is for wimps.** `[LAUGHTER]` What we want to do is, yeah, have it run in parallel. And there are lots and lots of hypotheses. Good. My manager is going to be happy, the client is going to be happy because we've tested so many things. And once we've done all of that, and it's always nice to watch code run, but beyond a point you just want to take a tea break. Once it's done, I'm not going to read it. I'm going to have it read it and figure out what the results are. Actually, no, I'm not going to have it read it and figure out what the results are because that's what it will tell me. I'm going to have it craft the exec summary that I can directly take to the client.

At which point you start thinking, wait a second, hold on. Anand, if this is what you are doing as a data scientist, I can do that as a business person. What are you there for? Clicking buttons? `[LAUGHTER]` Rest in peace.

Now, it errors out, at which point I could say, oh, in this sort of situation, I can come in and do some amount of correction. The reality is all you have to do is ask it to retry. `[LAUGHTER]` And it will solve the problem. So yeah, I'm not needed for that. Oh, but what if the code is wrong? You don't have the skills to validate it. Ask it to find faults in the code. What are you there for?

It this is going to take some time. But this is not the only way. And eventually I'll come back to this and there are more hypotheses than I would have liked, but eventually I'll come back and show you the summarize button. But this is one way we can kill off data scientists. Let's take the same thing, but we'll do it in a slightly different way. Vibe analysis. We say, Anand, you're showing this thing called Hypo Forge, you built it, you've got some knowledge. So maybe I as a data scientist can build similar things and show off. Yeah, good idea. But I'll show you how you can do something even better with ChatGPT.

The theme being vibe analysis, which is borrowed obviously from vibe coding. The spirit of vibe coding is coding as if code does not even exist. You just say this is what I want, it gets done, you don't look at the code, which is different from AI coding, where you're using AI to code and you at least look at the code. Vibe analysis is like that. You analyze as if the analysis itself does not exist. So what I'm going to do is pass ChatGPT some context, which is effectively this data set, which I've got as an aggregated zip file, about 2 MB zipped. And let's see. Yeah, here are some things that I might want to analyze. I got this from ChatGPT, obviously.

I've given you a data set and I'd like you to analyze this. See, what I want you to do is keep in mind the context is that there is a Naya Airlines that is launching in India and their operations head wants the data to tell them lots of stuff like which cities will make most money, which routes will always be full, at what prices people will happily pay, stuff like that. So let's do some exploratory data analysis. Give me a route leaderboard. Tell me what the growth is across different routes, what the seasonality is like. Do some network analysis, that's cool. Give me some weighted edges, centrality, all of that kind of stuff. Also do some fairness assessment, like comparing against per capita, you know, social welfare goodness which everybody's after these days. Also look at the variation between carriers, which carriers have entered, exited, and so on. But see, all of this, at the end of it, I want you to summarize for a business audience as poetry. Use poetic language, but also literally write poetry because apart from getting insights, I also want to be entertained. `[LAUGHTER]`

Let it run. The beauty of this is that I can do that while on a walk. Actually, I do that on a walk. Just talk to ChatGPT and feed it the data, which I can from my mobile. And it is going to eventually, once I click on this, analyze it and give me a poem that does the analysis. We'll wait for the poem. The waiting is the hard part and also the fun part. `[LAUGHTER]` Hard because you're impatient and you want to see the result immediately or I'm delivering a talk and it's awkward for me to have nothing to do. But fun because then I can play Minesweeper. `[LAUGHTER]` Which is what I often do. Actually, now it's becoming a little hard because these games are so addictive. I'm playing the game more often than I am actually coding. But nobody notices because I'm suddenly super productive.

Which is another point. **Until people discover that they can do all of this stuff by themselves, data scientists have a job.** `[LAUGHTER]` No problem about it. And this is a good technique. **Use AI. You don't have to say that you're using AI.** `[LAUGHTER]` Be practical about it.

All right. So it's saying a lot of things. It's saying, based on these hypotheses, I should explore joint ventures. Okay. Deploy off-peak charters, monetize customer data, blah, blah, blah. Finally, have you given me an executive summary at the end? No, it's still writing stuff. My God, itna analysis karta hai ye. But at the end of it, hopefully, it will, well, it certainly is already giving me specific actions. Do this, don't do this, etc. And here's an exec summary. Prioritize high-impact, strongly supported plays. Accelerate selective joint venture international gateways. Deploy off-peak charter ACMI programs. Now, here's the thing. As a data scientist, I don't understand a word of this. I don't care because the Naya Airlines guys, they're going to understand it quite well. They'll look at this and say, oh, that is insightful. And then there'll be one program manager who says, are you sure this is correct? All I have to do is go back, figure out which of those hypotheses is related to which item, copy-paste the result of that analysis and send it on email. Done.

This was an end-to-end cycle, by the way. We didn't do anything in the middle other than clicking buttons. Let's see where this ChatGPT is. The title of this, by the way, which you may not be able to read, says data analysis and poetry, which is not a bad title. And it's doing the same thing. It's writing code, it's running the code. ChatGPT can run code. It does a fantastic job of running code. I've uploaded short of a gigabyte, close to 500 megabytes of data into the container and it works absolutely fine. It cannot access the network, otherwise I would have told it to just scrape lots of stuff off the internet. But short of that, if you upload stuff, it can process it. That's your only bottleneck that it has, which I'll tell you how to get around in a different way. And it writes the Pandas code to do all of the analysis and, hmm, okay, it seems to have actually completed or has it? Okay. It's saying, blah, blah, blah.

I'm not really fussed about which direction you want to go in. Just do the whole thing. Don't ask me again and get to the answer. Come on, impress me. You are supposed to be a genius. Go. Keeping myself entertained is more important these days than keeping the LLM entertained.

So, you've got a sense of how vibe analysis might flow. What about cleaning the data? Painful, right? Utterly painful. Let's automate that. Again, it's possible to build tools that do this sort of a thing. And one of the tools that we've been playing around with is called Schema Forge, where you could upload a data set. I'm going to take the same data set, except it's an Excel sheet, that doesn't look right. Well, okay, let me upload one of these aggregated files as a sample. And yeah, click upload. Now, this is actually not great. One data set doesn't really look nice. So I'm going to skip this data set and I'm going to take one of the sample data sets that I have, which has pharma data. Now, why am I picking this data set? Because it has multiple tables. Right now, I'm too lazy to integrate all of the multiple CSV files into one. But this, by the way, is a crucial skill. The ability to very rapidly provide LLMs context. You still need to provide it context because it's a little hard for it at the moment to search through your, sorry, no, it's not hard. We are not very comfortable these days giving LLMs permission to our entire hard disk for it to scan through and figure out what is the right context. So providing it context is still a human task and will likely remain for maybe three, six months, maybe even longer. But until then, make sure you know how to build context. But what this does is goes through an Excel file, or in this case, a database with tables, and identifies what the column types are, guesses what the description of the columns might be, guesses whether it's personally identifiable information, guesses whether it might be a primary key, a foreign key, whatever, keeps running these one after another, item by item. And once it's done, it will be able to move to the second step, which is identify whether there are any data quality issues. I'm going to come back to this in a few minutes because we can also do exactly the same thing through vibe analysis.

So here's what I'm going to do. Take, okay, these are some of the results, but let's go to ChatGPT. Upload the same data set that I had earlier uploaded. And share. Yeah, okay.

I want you to do data quality analysis on this problem. Now, be comprehensive, be advanced, be really smart about the data quality analysis. See, for every data set here, I want you to list clear potential data quality issues, and make sure that these are non-obvious, non-trivial, mind-blowing. And while you're at it, suggest approaches to automatically fix these. Give it a shot. Remember, it can write code. It can see the results of the code. It does this ultra-popular agentic stuff, which means that it will keep running in a loop, calling these tools as often as it likes. So it can change direction midway. These are good instruction following models, which means that it stays course. A good measure of how smart a model is is how long you can give it, how long you can let it think by itself without it going off track. And that is increasing. These models can run for 10 minutes very comfortably and 10 minutes of these models is probably 10 weeks for many humans, at least for these kinds of tasks. So I'm going to let this run and switch back to see how our poetry is coming along.

Okay. "Routes of gold, a leaderboard sings. Delhi to Mumbai, the crown that it brings. Hyderabad rises with traffic so bright. Growth curves soaring, a new city's flight." Okay. So Delhi to Mumbai, fine. Good idea. Hyderabad, you may want to focus on. Not Bangalore. Interesting. "Seasons reveal their secretive way, festivals lift demand in May." Why May? What festivals are there in May? Why am I asking you? You are humans. `[LAUGHTER]` Let that figure it out. But you get the idea. We have the analysis. We can ask it to give the proper details of the analysis and figure it out. But also entertainment. It gives me, it does exploratory analysis and it says that in May there are festivals that are lifting it. Winter is actually less. And centrality whispers. Okay, I didn't understand this bit. Fairness, per capita journeys must also be free. Oh, okay. So the metro per capita flight or cost is relatively higher. That's something to explore. Indigo is way higher than any of the others. Stuff like that. Useful to know.

If, therefore, it can do the analysis, and let's go back to how it's gotten or where it's gotten on the schema side of things. It's identified all of these. It's probably put together the entity relationship diagram saying, okay, so there is a prescriptions table, which is connected to a drugs table, and there's clinician's data, patient's data, lab results. It's probably got it wrong, I mean, in some place or the other. But I can go back and review it. I can even ask it where have you gone wrong. But also have it then generate the DBT rules. Create all of the data quality checks that you would normally want to apply to this. Now, how does it know how to create those data quality checks? It can, from the data, observe things like, look, the value appears unique and consistent. So I'm going to ask for it to be not null, unique, positive integers. Perfectly reasonable. And there'll be 300 such checks for 300 columns, which I don't have to worry about. And even if I did it, I would make a mistake. Let it do the basic checks in an automated way. But some of these, like for instance, PT type, I didn't even know what it was, but it's gotten it as patient type classification code. And it looks like it can only be a single letter code and not null. That is useful. ETHFL seems to be ethnicity flag, and that's probably Asian, Latin American, African, and so on. So again, it's made a reasonable guess as to what the values can be. So that's the next level. It's even gotten as far as identifying relationships. So it's saying that here are the joins, here are the many-to-many, okay, many-to-one mostly, but there are some many-to-many joins as well, which it can start checking if the foreign keys are valid, which is useful.

Since it has all of that and it's able to generate the DBT rules, the next step is to have it create a package that I can download, a zip file which contains all of what I need to execute the DBT script, check the data quality, and it seems to have close to done that. It's saying here's a summary of all the rules which obviously I'm not going to read, I'm lazy. And allow me to download it? Has it... Oh, okay. Yeah. The run DBT locally button will download a package. This is a zip file that contains lots of stuff which you may not be able to see at the back, but among other things there is a setup DBT shell script which once I run will execute all of these and give me the full results. Meaning, data cleansing, or at least data quality issue identification, is now one-shotted.

What about the vibe analysis side of things? Did it do a good job at that or did it even do anything there? Let's see. Any interesting, okay, no, it's still working. But one of the things that I found when I last ran this exact prompt is that it found that there is, there are flights from Hyderabad to Hyderabad, direct flights. `[LAUGHTER]` The total number of drone certificates, the number of drones that are flying with certificates, is actually, the cumulative number is decreasing on some days, which means there are negative drones, anti-drones, whatever, anti-matter drones. Probably withdrawn certificates is my guess, but it's worth taking a closer look at that. On one day, we had more traffic than the entire world. It's theoretically impossible, but it's able to identify that, look, there is an anomaly somewhere in the data. And there was one day where there were more footfalls than there were people. So where did these magical new people come from? Examples of observations that come from the data. So you know, here are the kinds of problems that we might face with this data. But we are interested not in data cleansing, we are interested in modeling.

How would we do that? I'm going to do this very quickly. I'm just going to take one small problem and ask it to vibe model. Go to ChatGPT and add the same data set. And I'm going to say, "I want to predict what the traffic will be on any given day in the future across any route. Now, what I'd like you to do is build maybe multiple models and see which of these models are the best fit for specific routes. Make sure you do all the comparisons of quality. You know best what are the quality measures to pick, so make sure you pick respectable quality measures and against these, compare different models and pick the best ones. I don't have the patience to go through all of the routes. So maybe you can pick the top 10 routes and for these 10 routes, give me a table that says in rows, what are the routes, in columns, what are the models that you've picked, and in each cell, give me one quality measure so that I will know which is the right model to pick for these. Then do that work for me by actually picking those models, converting them into .pickle files, and let me download those pickle files. While we're at it, also create a Streamlit, no, create a Fast API application which I can run locally that can serve those models as an API so that I can deploy it later on."

**Why do I need to work?** `[LAUGHTER]` It will do it. And the reason I know it'll do it is because I've done this multiple times and it has not failed so far. But I am curious to know which model it picks, which models end up being better. So more than anything else, it's a great time to learn because this is teaching me in very, very practical terms what works, what doesn't work. The best kind of master class that one can have.

And then we need to explain this to people. Not fun, especially because people ask for crazy kinds of things. But at least I don't mind making charts on something as ugly as Matplotlib or something prettier like D3, which as a data scientist, I don't know too much about. But again, something that ChatGPT excels at. If it's about creating Matplotlib charts, and I'm not going to show you what the output looks like. The process is exactly the same. You say, these are the kinds of charts that I want, it will create those charts, reformat it as SVG, PNG, change the axes, change the style, ask for an Economist style. That works.

What's more fun is a workshop that I conducted for designers mostly. That workshop is called Prompt to Plot. And what we did was have people build data visualizations from data without any code. So they went to mostly Jules because at that point it was free. jules.google.com for those who may not be aware is, think of it as Cursor on the web. Now, one of the great advantages of this stuff on the web is you can code without being near your computer. Now, for many data scientists, that's not a problem, but I unfortunately also have a business role and I end up traveling a fair bit. So at airports, this is what I do. During my morning walks, this is what I do. I go to Codex or Jules and give it instructions. I'll show you how maybe not in this session, there's a vibe coding session that's happening, so I'll show you how where we just talk to Codex, just like I talk to ChatGPT, tell it what you want, it will build the code, it will push into your GitHub repository. You can from there directly deploy it and the whole problem is solved, which is exactly what these designers did. And without knowing how to code, here are some of the prompts that they used. Let's take a look at some of the stuff that they made. Where did this go? Yeah.

So this is something that, who was this? Rishabh created one-shotted, I think, given the Spotify data set. He created a series of interactive data visualizations that shows that there is a trend of increasing audio loudness in music, that some artists have a very distinct sonic signature. And if you want to compare, let's say, AR Rahman, or look at the signature of AR Rahman against the average, he tends to have a lot more acousticness and a little less liveness. I'm not sure you can see that, so I'll, yeah, move it. Or pick, any recommendations on artists? Ilaiyaraaja? Okay. Is Ilaiyaraaja around? Nope. Who Do Gurus, whoever they are, have a lot of energy and a lot of valence. Good to know. You get a feel for the kinds of things that they're doing and that radio-friendly songs, they should be around three minutes, but how does it vary across genres? It looks like minimal techno seems to have the longest duration, whereas progressive house has a little over 3.6, not that many that are below the three-minute recommendation. And so on. It goes on and on. And the fact that a non-developer can create these kinds of data visualizations is impressive. The fact that the person was a designer, obviously helps. They have an aesthetic sense and know what to ask for, which is leading towards the kinds of things that we as data scientists might want to go towards. There are stories around books, there are stories around, yeah, more book stories, lots of stuff. But the crux of it is creating charts is no longer a difficult job that a data scientist needs to worry about.

Okay. What about deployment? Here's a prompt that I ran earlier. I'm not going to bore you with running the same kinds of prompts over and over again. But this prompt effectively was saying, "Look, build the best model," more or less like the prompt that I gave earlier, "and give me a Streamlit app that can take the route as an input and the date and tell me the forecast of what the traffic will be on that route, on that particular day." It's done the job. And I actually spent some time going through the models that it picked. It was not a bad choice. Could it be improved? Maybe. And with a little bit of back and forth, we'd be able to figure it out. But eventually, it somewhere... It's thinking a lot. About below. Yeah, it gave me a zip file.

Now, I unzip this file. And yeah, here is the zip file. It has a models.pickle, it has a requirements.txt. So I'm going to say `uv venv`, and then `source that uv pip install requirements.txt`. That's installed all the requirements. Now, `streamlit run app.py`. And that has built, deployed, and is probably, yeah, going to run the application. It's loading the models. Okay. So now, for Agartala to Kolkata, let's pick something else. If I want to travel to Bangalore from Ahmedabad, let's say on Monday, the traffic is likely to be 37,000. Okay. For Naya Airlines, that's useful information to know. Next month, is it going to be higher or lower? My prediction, because we're probably going towards the holiday seasons, it might be higher. Overall, everywhere. 35,000. Okay. Not much of a difference. October, I still predict higher. 42,000. Okay. So it kind of aligns with the very crude intuition that I have about what the traffic route looks like. And I could do the same for different locations, do a spot check. And this is the kind of thing that a data scientist will have to start doing in the future. The system can code. Does it make sense? The common sense, the domain knowledge is something that will be useful to have because then I can reduce the number of iterations. The business user will probably struggle to make changes to this if something goes wrong. The beauty of vibe coding and vibe analysis is that when it works, it's fantastic. When it doesn't, it is not much better, perhaps even worse, than doing it from scratch yourself. So that is a problem, but factor remains that we can effectively deploy applications using a mechanism such as vibe coding.

What about analysis? You heard a talk in this room, probably, on how you can generate synthetic data. But the manager comes over and says, as they usually do, here's this data set, I want you to anonymize it. The classic approach is we generate random fake data. Not a good idea because if you analyze random fake data, you get no results. There is a simple alternative, and that is to use a prompt like this. You say, generate realistic fake data for some domain. You list the columns, I mean, you ask it to, and let's say this is fake data for our lines. You say, list columns and the distribution for those. Make a guess on how it will behave. And pick five hypotheses. If you have hypotheses, then introduce them and say that this is the behavior of this data set. Now generate random rows, but make sure that the data aligns to these rows, and then test the hypothesis, just making sure that you've gotten this right, and let me download the data. If we put that into ChatGPT with a specific domain, which for some reason I missed, so let's copy this again and paste it and add lines. Have it generate the hypothesis. Now, I am curious what hypotheses it will come up with. Watching the thinking is fun, so we will watch it for a few seconds, or maybe we'll come back to this and, okay, let's see. It's following my minimalism and CLI preference when implementing this. Okay. That is not a bad thing. Ah, routes, fares, demands, delays, seating, weather. That's not a bad data, that's not a bad set of columns. Monsoon months have higher delays. Okay, that is an interesting hypothesis. Longer flights should have a higher base fare. That's useful. Holiday periods show higher load factors. So it's thinking sensibly. At least it has the average common sense that you would expect a reasonable human to have, which of course data scientists lack, that's a different story. But these do have. And it will create the hypotheses and generate the data set.

What that means is that anonymization now is no longer a random exercise, it is an insight-driven exercise that we can factor in. We've left a bunch of threads pending. So let's go back to those and wrap those up. This is the insights, we've covered that. This is probably the cleaning thing which is stuck on my screen, so we will come back to that. They're all stuck on my screen, so basically I've overloaded the browser. This we've covered. This is still running. Okay, looks like I'll have to come back to these later or not at all. You can see the slides and I will eventually link to these. But let me end with takeaways.

**It's the tasks that are going away.** The talk title was obviously provocative, intentionally so, generated by ChatGPT. The learning, in my mind is, there are a bunch of things that we will need to do less of. Exploratory data analysis, that the system can handle. Creating the basic scaffolding, documentation, unit tests, doc strings, it can do a pretty good job of. Don't waste your time on that. Don't learn that, don't teach that. Modeling, going as high as AutoML, it can do that. The entire coding stuff is something that you don't need to worry about. What features to choose, what model to choose, what parameters to choose in the model, what are the metrics that we should use to evaluate it, it'll do that stuff. Writing the code, you need to provide it the specs, the LLM can fill out the remaining 80% of it. How to explain it? It can craft a story better than you or I can, in any case. We leave that to the LLM. Filling out forms and administrative stuff is now increasingly becoming easier. So these days, if somebody asks me what have you been doing over a particular period, I just look at my Git commits, pass it to an LLM, and summarize it. In fact, I've converted that to a weekly podcast now which explains what I've been doing and that's on my GitHub profile. Entirely automated, free, and effectively digital exhaust, as it's called, the stuff that we emit as byproducts, are now productizable, which is cool.

**So what do we need to focus on? What are the tasks that are coming our way?**

**Leadership.** Not at the vision level perhaps, but at least team leadership. You now have, whether you like it or not, a team of intelligences. Some of them are not human. That's all. But it's still a team. And you need to at least tell them what is the goal that they need to work towards. In other words, what problem should they solve? You have to pick the problem that you allocate to it. And you have to know which problems to give it. There are some where you must give the problem to a human. Impossible client who doesn't know what they want, give it to a human. Not because the LLM can't do the job, but because they will not interact with the LLM. On the other hand, well-defined task, good client, fairly flexible, give it to an LLM.

**How do you frame the problem?** Now, earlier, we could just fumble around, take the next step, is this going okay, if not, is it going the other way? We still can, there's nothing wrong with that. But what happens is each step taken by the LLM is ultra-fast. Each step the human spends to review it is ultra-slow. So where we could have gotten a 20x, 30x, 50x improvement in time, because we are sitting in the middle reviewing it, we are only getting a 2x, 1.5x, 1.2x improvement and leaving potential on the table. And therefore the ability to think further, scope it right, and prompt it right makes a big difference in reducing the number of iterations.

**Making evaluations a critical part.** If you can have the LLM figure out whether it's going in the right direction by having it run automated evaluations, perfect. It can course correct and therefore run longer by itself.

**Can we define things that do not change over time, that do not change at all in this system?** Are there invariants? Are there structures? Are there predicates? Are there truths or constraints that if you provide, it knows it's doing something wrong when it goes outside of that.

**Can we provide it more verticalized context? Can we give it more domain-specific problems?** And why is that? Because the more domain-specific it is, the more defensible it is in the LLM era. If there is something generic and horizontal that LLMs can solve, ChatGPT will solve that. OpenAI will solve that. Gemini, Google will solve that. Many people will solve that. The advantage that most companies have, therefore, is their data, their process, their own stuff, which is broadly termed domain, and going deeper in that area is certainly an advantage.

**Taste.** The ability to validate. So the role of an LLM auditor is one that is going to improve because if you can poke holes the right way without necessarily knowing all of the stuff that the LLM has done, but very quickly narrow down to the errors and tell it to correct this, or tell people this is where it's going wrong, it has value. Or, yeah, taste. Narrate it in a way that has value for the audience without them having to go through a boring LLM output. That has value too.

**In short, what I believe is that your role has changed. The tasks of a data scientist that existed in 2023, possibly 2024, are not what they are in 2025 onwards. Rest in peace, old-world data scientists. Welcome, new data scientists.** Thank you. `[APPLAUSE]`

ChatGPT is probably still going on, but we have time for questions. Anybody has questions, please raise your hands. We'll have a volunteer come up to you with the mic. We have six minutes, so yes, we can take some questions. You can start shouting, then we'll worry.

**Question**: How would we adapt, because if someone from the senior roles asks, why can't you do this in five minutes since the same thing could be done?

**Answer**: Question one, if your boss says, why can't you do it in five minutes, ChatGPT can, how do you answer it? Question two, how do we adapt? For question one, ask ChatGPT. `[LAUGHTER]` It is increasingly becoming the answer. I could give you something and I share based on my experience, but it probably has more experience. And what I usually do is take my experience, which I store as notes, feed it into ChatGPT and say, look, here are my notes. The other source that you have is the entire internet, all of the research and so on. Here is my question. Now you figure out the best answer. And it will absolutely do a better job than I can. Which begs the question, what am I doing here? I'll tell you one or two anecdotes which I will eventually share into the pool of the internet, which it will absorb and give you an even better answer on. But rule number one, gauge your manager's or client's level of understanding. If they are less mature, don't even mention AI. `[LAUGHTER]`

Number two, swallow your pride and fail a few times. Sir, I am so sorry, I didn't know how to do this. This is becoming very hard. And you have to do this with great gusto. You have to send emails at 3:00 p.m., scheduled at 3:00 a.m. scheduled, obviously. But they have to be very realistic. And if possible, write an LLM application that will send a realistic email that will respond to something. The idea being that you project it as being harder than it really is. Third, get expert advice. Get experts to also say, oh, it is really hard. The best will be, have someone else do it. Say, I'm not able to do it. Here is a really smart person. They will be able to do it. Set them up for failure. Preferably someone whom you don't like. `[LAUGHTER]` That works really well. Read Machiavelli. It's a good source for these sorts of things. This is stuff that ChatGPT may not tell you. So I'm trying to stay to core competence because it thinks a little more nicely. But on a serious note, the managing of expectations is best done by telling people, I am skeptical, but I will follow your direction and fail. It really happens anyway. You don't have to try very hard. How should you adapt? Experiment. The thing to know is that we know nothing. Every single thing that we've learned over the last 10, 20, 30, 40, 50 years is now changing. Question every assumption. And therefore, as a rule of thumb, A, I practice asking 50 questions to ChatGPT every day. I'm struggling. And to people, I'm listening a lot more than I'm talking. This is the maximum, the amount that I'm speaking right now is roughly going to be one-third of what I'm going to be asking outside. Learn. That's about it.

**Question**: Thank you for the wonderful presentation, Anand. I will probably ask my question to GPT, but also wanted to ask you. So you were saying that you could execute the code as well with GPT, right? Can you talk about a bit on how?

**Answer**: The feature is called code execution. You tell it to write and run code, it will run code. It runs it in a sandbox which does not have network access, but otherwise can run Pandas and the likes. Gemini has that feature as well. Claude also has that feature. Gemini's is okay. Claude's is more in the browser, therefore slightly limited, but powerful on the front end.

**Question**: Could you name some of the best in class for generating the models and the code?

**Answer**: Best code generator models. Model wise, Claude Code remains the best, but GPT-5 is pretty good, followed by Gemini 2.5 Pro. But application wise, it depends on taste. If you want something that's web-based, which is what I like, then Claude, Codex, ChatGPT Codex is higher than Jules in some areas. It follows your instruction really well. UI and style, Jules is slightly better. Cursor now has an agents feature which I have not tested. If you want command line, Claude Code, Codex, Gemini Code CLI. If you want VS Code sidebar kind, then Cursor, Windsuf, the rest.

**Question**: And how does it actually work? So these are trained using Codex?

**Answer**: How are these models doing a better job than just putting it into ChatGPT? Codex is specially trained. Claude's models generally tend to do better on code. On top of that, the tools like Cursor, they understand how to use these models better and are incorporating the logic, plus they run multiple times correcting errors.

**Question**: I just have one comment, that I'm not a data scientist, I'm a domain expert. And I just can assure that what you are doing now, what you did, I can do on my own. **So if you data scientists do not adapt to the new way of working, then your job will be done by the domain experts.** This is my message to you. `[APPLAUSE]`

**Answer**: **We have literally seen a few people getting fired in this room.** `[LAUGHTER]`

**Host**: Now I know why you chose the name R.I.P. Let's have a big round of applause, ladies and gentlemen. Thank you so much. `[APPLAUSE]`
