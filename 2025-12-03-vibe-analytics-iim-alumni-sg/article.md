# The Oracle in Your Pocket: How a WhatsApp Group Revealed the Future of Intelligence

On December 8, 2024, in a conference room somewhere in Singapore, two men stood before a group of senior business executives and issued what sounded like a preposterous challenge: "Take the largest financial decision of your life," one of them said, "and let an AI make it for you."

The man who said this—S. Anand, though his friends call him Bhalla—wasn't being provocative. He was being literal. Two months earlier, he had done exactly that. Using nothing more than a detailed prompt fed into ChatGPT, he had made what he described as "the largest investment of my life." Not after consulting the AI. Not after using it to check his work. He had simply asked it to analyze his risk profile, survey the macroeconomic landscape, run sensitivity analyses across multiple scenarios, and tell him where to put his money.

And then he did it.

"I cross-checked it," Anand said to the room, almost apologetically. "I mean, it is a lot of money." But the AI's reasoning had been so sound, its logic so comprehensive, that he had followed its recommendation to the letter. He paused, then added the kicker: "And the average personal financial advisor is now beaten by the majority of the top AI models."

The room went very quiet.

## The FOMO Epidemic

Here's what you need to understand about that room: it was full of people who had spent their entire careers being the smartest person at the table. IIM graduates. BCG consultants. Founders of multi-million dollar companies. CFOs of major corporations. People who had built their professional identities on being fast learners, quick studies, the ones who could master any topic under the sun.

And they were terrified.

"Let's be honest about this," said Nirmal—Anand's co-presenter and a man who co-founded Fractal Analytics in 2000, back when most people still thought "analytics" meant Excel pivot tables. "Senior leaders are embarrassed to learn. With all due respect, senior leaders do not open up their laptop; they probably get other people to open up their laptop."

The room laughed, but it was the nervous laughter of recognition. One by one, they began to articulate their fears: How do you keep up when everything changes every day? Will humans be completely replaced? How do you separate hype from reality? One man confessed he'd recently told someone: "Sir, these days I left real estate and started AI."

But here's what makes this story interesting: these were exactly the wrong fears.

The revolution that Anand and Nirmal were describing wasn't about keeping up with technology. It wasn't about learning to code or mastering the latest model or understanding the difference between GPT-4 and Claude Opus. The revolution was about something far simpler—and far more profound.

It was about asking the right questions.

## The Autocomplete Theory of Everything

"What is an LLM fundamentally about?" Nirmal asked the room. Then he answered his own question: "You've seen autocomplete on your WhatsApp? Imagine I can extend it to the next sentence. The next paragraph. The next page. The next 450-page novel. And the next four such novels."

He paused to let this sink in. "It's an autocomplete model which predicts the next four novels from your prompt. That's all it is."

This might sound reductive—dismissive, even—but it's actually the key to understanding everything that followed. Because once you grasp that large language models are fundamentally just very, very good at predicting what should come next, you realize something counterintuitive: **the bottleneck isn't the AI's capability. It's your ability to give it the right starting point.**

Or, as Anand would later demonstrate with a simple tool he'd built, your ability to combine two random ideas and see what happens.

The tool was called Ideator. Anand pulled it up on screen and showed the room how it worked: he had been collecting notes for years—fragments of ideas from technology conferences, creative prompts from the famous Oblique Strategies deck, principles from Daniel Kahneman, tips from colleagues. Thousands of these fragments, all indexed, all waiting.

He clicked a button. The system randomly selected two concepts: "Humanize something free of error" and "Never delete data. Mark it for deletion instead." Then it fed these, along with a prompt asking for a "life hack," into ChatGPT.

What came back was something called a "Wickedness Dial"—a conceptual tool for adjusting safety rules based on context. The idea was half-baked, possibly useless, but that wasn't the point. The point was that it took 30 seconds to generate, and Anand had already moved on to asking Claude for another version, and Gemini for a third.

"When in doubt, ask AI," Anand said. "When not sure whether you're in doubt or not, ask AI."

A woman in the back raised her hand. "But how do you know if it's accurate?"

"Ask AI to cross-check itself," Nirmal shot back. "AI cannot lie." He paused, then corrected himself with a grin. "Well, it can. I asked for a top 10 list of 2074 Indian science fiction cyberpunk stories. Number five was fascinating—I couldn't find it anywhere. It not only lies, it lies beautifully." The room laughed. "So why are we charging it with veracity? It's an autocomplete model. It's a potential autocomplete. That's all."

## Maps Trump Shovels

This is where the real thesis emerges, and it's deeply counterintuitive.

During the early days of the internet boom, there was a famous saying: "During a gold rush, sell shovels." The idea was that the real money wasn't in finding gold—it was in selling the tools to the prospectors. This became Silicon Valley gospel: build platforms, create infrastructure, sell picks and shovels.

"That's the wrong metaphor in the age of AI," Nirmal told the room. "The right metaphor is: **If you want to find the treasure, you need a map.**"

He let this hang in the air for a moment. "Maps trump shovels. What does that mean? You need to know the metadata, the template, the recipe, the structure of the lay of the land."

Here's why this matters: every person in that room had spent decades building deep domain expertise. Healthcare. Finance. Publishing. EdTech. Corporate strategy. They knew their fields intimately—the unwritten rules, the edge cases, the questions that separated novices from experts. This knowledge, they had been told implicitly for the past two years, was about to become obsolete. The AIs were coming for their jobs.

But Anand and Nirmal were saying the opposite: **Your competitive advantage is not deep tech. Your competitive advantage is deep domain expertise.** The AI doesn't know healthcare or finance or publishing. It knows the mathematics of language. It knows patterns. It knows what words tend to follow other words.

But it doesn't know which questions matter in your field. It doesn't know what "good enough" looks like in your context. It doesn't know the difference between a solution that looks right and one that actually works.

You do.

"Take your Deep Domain Expertise," Nirmal said, "and build the map around that. That's what you're good at. And how do you do that? Ask AI. 'Perplexity, I want to build a map for this domain. You ask me questions so that I can start building that map.' Have a conversation."

He paused, then added: "AI demands a seat at the table. A brainstorming partner."

## The 239-Second Challenge

While Anand was setting up his next demonstration—a live analysis of his IIM alumni WhatsApp group—Nirmal casually mentioned a contest he'd been running with his students: who could create a prompt that made an AI "think" the longest?

"The maximum time is 239 seconds," he said. "If somebody finds a prompt that can beat that, we'll give you a prize."

This is revealing in ways that might not be obvious. What Nirmal was measuring wasn't the complexity of the prompt—it was the depth of processing required to answer it. He was, in essence, trying to find the hardest questions humans could ask machines. And 239 seconds—nearly four minutes of pure computational thought—was the record.

Think about what this means. These models can write a novel in 30 seconds. They can analyze a financial report in 45 seconds. They can generate working code for a complex application in two minutes. But give them the right question—one that requires reasoning across multiple domains, considering edge cases, weighing trade-offs, and synthesizing novel insights—and they'll think for four full minutes before responding.

The bottleneck, in other words, isn't processing power. It's question quality.

## What 122 Classmates Can Teach You About Intelligence

Anand uploaded a text file to Claude—three years of WhatsApp messages from his alumni group. Then he typed a single word: "Analyze."

What came back was, in its way, extraordinary. Claude identified 122 active members, mapped their geographic distribution, spotted communication patterns, and categorized conversation themes: professional networking, personal support, nostalgia, reunion planning. It identified coordinators and organizers. It flagged problems—one member had been posting multiple blank messages (actually deleted messages), creating tension in the group.

But then Anand did something interesting. He wasn't satisfied with the basic analysis. So he tried again, this time with a different approach: "I'm sharing the transcript of my WhatsApp group. I'm part of the group but not really actively reading. Catch me up on recent threads, and give me interesting stuff—not obvious things, surprising things."

Then he tried a third approach. He asked Claude to write the analysis in the style of Malcolm Gladwell—"opens with a vivid, unexpected anecdote about an obscure person or event, then zooms out to reveal a counterintuitive thesis."

What emerged from this third attempt was remarkable: a 3,000-word narrative that began "November 29th at 3:40, a man named P Chidambaram—not the Finance Minister but my classmate—typed 13 words into a WhatsApp group..." It turned a mundane group chat into a story about community dynamics, participation inequality, and the invisible rules that govern digital spaces.

"I'm going to spend this evening reading this," Anand said, "because it is way more interesting than the WhatsApp group itself."

But he still wasn't done. He took out an even more detailed prompt—one he'd refined over years of doing data analysis. It ran to several paragraphs and included instructions like "Look for the unspoken—what are people NOT talking about?" and "Find the outliers—who is speaking too much or too little?" and "Look for conflicts, even subtle ones."

The results were immediately actionable: "Ask Shivani how her job hunt is doing. Ask Pawan and Vohra about their mountain trek." These weren't abstract insights. They were conversation starters—specific, contextual, useful.

"This is something I learned," Anand said, pointing at the screen. "What prompts or texts I can use to jump back into the conversation."

## The Wickedness of Clean Data

At one point, a man named Suresh raised a common concern: "Garbage in, garbage out, right? If the data is dirty, the analysis will be wrong."

Nirmal's response was swift and definitive: "Let's get very candid about this. **50% of data in the world is always dirty.** There is no concept of clean data. Show me an organization which has clean data, I will eat this pen."

This is the kind of truth that makes people uncomfortable because it violates everything we've been taught about data analysis. We've been told: first clean your data, then analyze it. First establish ground truth, then build models. First get everything perfect, then act.

But Nirmal was saying the opposite: **the dirtiness is the data.** The missing values, the inconsistencies, the edge cases—these aren't bugs to be fixed. They're signals to be interpreted. And AI is remarkably good at working with messy, imperfect, real-world data precisely because it doesn't require perfection. It requires context.

"AI is not hallucinating," Nirmal continued. "**Hallucination is the fact that you've asked a question and it has used the context that 'orange' is a color instead of a fruit.** So there are paths where there is some ambiguity and it's taken the other path which you were not interested in. If AI is not hallucinating, your prompt is not clear."

He paused for effect. "Let's be clear, what are we charging AI with? Why are we charging AI with accuracy? It's an autocomplete model."

## The Real Revolution

Here's what makes this story counterintuitive: the revolution that Anand and Nirmal were describing has nothing to do with artificial intelligence becoming smarter. It has everything to do with humans becoming better at articulating what they know.

For decades, expertise has been locked inside people's heads—tacit knowledge that couldn't be easily transferred, documented, or scaled. The master craftsman who "just knows" when the timing is right. The doctor who "has a feeling" about a diagnosis. The investor who "trusts their gut" on a deal. This knowledge was real, but it was also stuck.

What AI does—and this is the thing that people consistently miss—is provide a mechanism for externalizing that knowledge. Not by replacing the expert, but by giving them a tool that can take their half-formed intuitions, incomplete explanations, and contextual understanding and turn them into something concrete.

"Your competitive advantage—Alok is an expert at healthcare," Nirmal had said, pointing to someone in the audience. "There's nobody like him in Singapore, and the world I'm sure. Debi used to consult for CFOs; she can teach any CFO. You take that expertise, that Deep Domain Expertise, and build the map around that."

This is why Anand had felt comfortable making that large investment based on ChatGPT's recommendation. Not because the AI was smarter than a human financial advisor—though it might be—but because he had been able to articulate his constraints, preferences, and context in such detail that the AI could reason about his specific situation. The prompt he'd given it ran to several paragraphs and included his risk tolerance, investment horizon, tax constraints, and macroeconomic assumptions.

The AI didn't replace the financial advisor. It replaced the need to translate his internal model into something another human could understand well enough to advise him.

## The Empty Chair

Nirmal had made an unusual request for the workshop: "One of the things I'd requested was one empty chair at every table. Call it AI. AI has a seat at the table. AI is a first-class citizen."

The metaphor is perfect. Not AI as a tool you use. Not AI as a system you consult. AI as a colleague—someone you argue with, someone you ask to critique your work, someone you tell to cross-check themselves when you suspect they're wrong.

"When in doubt, ask AI," Anand had said. "When not sure whether you're in doubt or not, ask AI."

But here's the deeper insight: this only works if you know enough to know when the AI is wrong. It only works if you have the domain expertise to say "wait, that doesn't make sense" or "try again with different assumptions" or "now verify this against actual data."

In other words, it only works if you're the expert.

Three years ago, a small virus held the world hostage, Nirmal had reminded them. "Three years the whole world was held to ransom and we couldn't do jack shit about it. There ARE problems to be solved." He was pushing back against the fatalism that nothing important remained to be discovered, no meaningful work remained to be done.

But he was also making a subtler point: **the problems that matter are the ones that require deep context to even recognize as problems.** The question isn't whether AI will replace human expertise. The question is whether humans will learn to use AI to leverage their expertise in ways that were previously impossible.

## The Speed of Your Mouse

Near the end of the session, Nirmal told a story about attending a summit where someone wearing a "Zen tie-dye t-shirt" had given unusual advice for keeping up with the avalanche of AI developments: "Change the speed of your mouse to 5x." The idea was to capture everything quickly—screenshots, notes, quotes—without trying to analyze it in the moment. "Analyze later. Don't analyze right away."

This might be the most important practical insight from the entire workshop. The instinct when confronted with information overload is to try to process everything in real-time. To understand it as it happens. To keep up.

But what Anand and Nirmal were teaching was the opposite: capture first, analyze later. Let the AI do the synthesis. Your job isn't to keep up with every new model or technique. Your job is to know what problems you're trying to solve and what good solutions look like in your domain.

"India is going to lose its advantage—labor arbitrage of programmers," Nirmal had said early in the session, "because code is going to be replaced by well-written word documents."

At first, this sounds dire. But look at who was in that room: people who had spent their entire careers writing well-structured documents, creating specifications, articulating requirements. They weren't behind. They were actually, weirdly, ahead.

The revolution isn't about who can code. It's about who can think clearly enough to explain what they want—and who knows their domain well enough to recognize when they got it.

That night, after the workshop ended, Anand did exactly what he'd promised: he spent the evening reading Claude's Malcolm Gladwell-style analysis of his WhatsApp group. The one that began with P Chidambaram posting 13 words at 3:40 PM. The one that turned months of mundane chat logs into a story about community, norms, and the invisible rules that govern how we interact.

It was, he said, way more interesting than the original conversation.

Which raises an interesting question: if AI can take our messy, everyday data and find patterns we didn't know were there—stories we didn't know we were telling—what else are we missing? What other maps do we need to draw? What other questions should we be asking?

The oracle isn't in the cloud. It's in your pocket. You just need to know how to talk to it.

<!-- https://claude.ai/chat/98212b25-27e0-42c0-9f71-3e365a638a83 -->
