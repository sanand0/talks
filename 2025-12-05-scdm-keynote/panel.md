# Panel Discussion

**Arnab**: Am I audible? Yeah, working well. Thank you so much for the very insightful presentation. This combination of presentation and wonderful demonstration is exactly what a lot of folks were trying to understand—how does it work in real life and how do we connect that to some of the use cases on a day-to-day basis of what we all are working on. Let's have a brief chat on a few things. You have touched upon a few important use cases, right from the informed consent protocol, the cost evolution, anomaly detection...

**Anand**: I must interrupt you there. I have no idea what these words mean, by the way. I just spent a late night and an early morning searching them up.

**Arnab**: I’ll come to that. But first, let's explore the huge LLM hype: the promise versus practice. What according to you are people overestimating currently versus underestimating? What are a couple of examples that you can help us with?

**Anand**: In general for technology, **in the short run, we tend to overestimate or hype stuff, and in the long run, we tend to underestimate what it can do.** There is one particular measure of how long an LLM can think by itself which is one of the popular measures that this organization, METR, is looking at (Reference: https://metr.org/blog/2025-03-19-measuring-ai-ability-to-complete-long-tasks/).

**Anand**: In 2020, LLMs could think for about 15 seconds by themselves. As of 2024, 9 minutes is reasonable. As of the end of 2025—almost getting to 2026—**2 hours and 42 minutes is how long you can leave a model alone by itself and have it do the work.** Look at the shape of this curve. If there is one thing that humans underestimate, it is compounding. It is not in our blood; we just do not get it. This is compounding to the core.

**Anand**: What this is implying is that models are going to be able to autonomously think for hours, days, weeks. Now, what it was able to do in two and a half minutes is what would have taken me about four hours or ten hours. And if it can think autonomously for that long, the scale of the problems that it can tackle is something that **we are grossly underestimating because we lack the imagination.** The overestimation is easy. You just pick up any website and talk about how it's using AI for X, and yeah, when you try it, it won't work. That's the bad news. But personally, if we projected a lot of the hype to two years down the line, it's not hype.

**Arnab**: I'll touch base on a very important question which is probably at the top of many people's minds: the future data managers, the future clinical data scientists. A lot of them, a lot of us, would like to get started with LLM as soon as possible, but sometimes we are not able to figure out how. What's the right mindset that people need who are about to start with LLM and application of AI for the first time? And in the future, for a clinical trial expert—not necessarily a developer or a programmer—is coding even necessary?

**Anand**: Let's take those. How does one go about starting the journey? My father learned most of his electronics by watching me operate a tape recorder at the age of two or three. He bought me my own tape recorder. A fair bit of the robotics that I'm learning is from my daughter, and she's the one tinkering around with all kinds of stuff; I know nothing of it. **Kids are faster than us. They are AI native today. So an easy way is to learn from your kids.** And teaching is a great way of doing that.

**Anand**: So, one of the reasons I'm actively teaching is not as much to impart education—that is a byproduct—it is to see what these people are doing and learn from it. When I give an experienced developer a task, they take a week. The same thing, when I gave it to an intern a couple of months ago, he finished it in half a day and said, "Anand, I have two variations of this, I'm not sure which is better, can you guide me on this?" You're done? Yeah, you told me to finish it, no? He didn't know it was impossible, so he went ahead and finished it. I realized that there are a few models that are far better, a few tools that are far better than anything that I had used, produced a style that was far greater, and now I've started using them. That's one way: **learn from the young.**

**Anand**: The second way is to pay $20. That's all it costs to get a ChatGPT Pro account, or a Gemini Pro, or a Claude Pro account. It costs about as much as say a Netflix or Prime Video. The ROI on that... **I have not seen an investment with higher ROI ever in my life.** And that's saying a lot; that includes the modem that I purchased in 1996 or 1995 which took me to the internet. Even compared to that, this is higher ROI.

**Anand**: At work, it's a little trickier because we are operating in an industry that is regulated. So wait, you don't necessarily need to force things. Now, there will be pressure. Pressure will come from two directions. One from the bosses saying, "Everybody is using AI, I want to go tell my peers that we are also using AI, so use AI." Pressure will come from your peers saying, "Oh, I did X." I didn't do X. Don't worry. Use it for what you like. Putting pressure on people... and please don't pass on that pressure to other people. It's enough that AI can do, and people will figure it out by themselves. It's going fast, it's going crazy; peer pressure is the last thing that we need.

**Anand**: To the other question: do we need to know programming? No. I was programming. I didn't write a single line of code. **I've been programming every day for the last 30 years. But in the last three to six months, it's changed. The language that I've been programming in is largely English.** I can't remember the last time that I really actually coded by hand. It must have been a few weeks ago, but it's becoming increasingly rare. And you can do data analysis with it, you can do image reprocessing with it—and by "it" I mean English—and you can do regular application development with it, you can do maintenance with it, you can move data around from one place to another, you can clean up data.

**Anand**: The datasets that you just saw [in the demo], these look very realistic. They are all synthetic and they were all created by ChatGPT. I didn't create the prompts that created them either. I have no understanding of the domain, so I just took the transcript of the call that I had with [Prabhakar] and put it in there and said, "Look, somebody said something. You know this domain better than I do. Give me some prompts for data sets." It gave me the prompts, which I pasted. Not only do I not need to know programming as a domain, **I don't need to know any domain. I can pick what area I am an expert in, go deep into that, and for the rest, I use LLMs.**

**Arnab**: Thank you, Anand. That prompts me to another question, probably one of the hot takes in the questions coming from the audience. It's much easier when you're demonstrating and not with the real-life scenario, especially in clinical research as an industry which is highly regulated, where data integrity is at the top. We need governance, we need security. And we often hear about the risk of bias, the risk of hallucination, and there is a "black box syndrome" that a lot of us already have because we tend to think that we do not understand how the LLM works at its best. So what's your advice to the people who are probably going to use LLM for the first time in the large scale to deal with this, and also for the leaders?

**Anand**: **Use it for offense, not defense.** See, the problem is, if an LLM gives a result that it's supposed to defend or you're supposed to defend, you have a problem. Use it to attack instead. A human has come up with a result. What are the possible biases in that? Have it find out. A human has come up with a whole series of analyses. What are the possible errors in that? Have the LLM find out. In short, **the safest way you can use LLMs today is for validation.** Because there is zero downside. There is only upside. It is practically free. You can ignore it, no harm.

**Anand**: Now, even to do that offense, you need to send the data to a model, and you may not have permission to send that data to the model. Don't. Instead, what you can do is send the structure of the data saying, "I have these 40 columns. You write me a program that can do the cross-check." Run the program locally. Now, your problem may be you don't know how to run the program. And that's where these coding models and coding agents come in.

**Anand**: I've been instructing our finance team... actually, I'm going to show this email if I can locate it quick enough. This was from our finance controller. He is, almost without doubt, the most conservative person in our organization. And he said to his team: _"Team, please use this opportunity to install Code Interpreter AI as per the recorded demo. This is very powerful. Yesterday I tried it for two data requests and the result was fantabulous."_ Now, for a conservative Chennai-ite to use the word "fantabulous," it just made my day.

**Anand**: And this is a person who certainly cannot code. But not only is coding, he's vibe coding on data on his machine which is about as sensitive as it can get for our organization—it's the financial records that have got to go through a huge amount of scrutiny. And he is now comfortable because **he knows that the data is not going to the model. The code is coming from the model.** And Code Interpreter was executing the code on his machine. He is not typing the code to run the program. He just said "do this," and it wrote the code, it downloaded the code, it ran the code, it provided the results—all done. Short answer: **No, we don't need programming.**

**Arnab**: Thank you, Anand. We are unfortunately running a little behind schedule, so we will end the chat here. But I'm hopefully you are available for some time, so if you have any further questions please catch hold of Anand. And thank you so much for the wonderful insights.
