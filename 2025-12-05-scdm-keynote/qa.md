# Q&A

<!-- Q&A: https://gemini.google.com/u/2/app/6f20f79d13f67671 -->

- **Anonymous (09:19 AM): How do we deal with AI hallucinations?**
  - We treat AI as a reasoning engine, not a knowledge base, by using **RAG (Retrieval-Augmented Generation)** to force it to cite sources from your documents. Crucially, for data tasks, we ask the AI to write and execute **Python code** (which is deterministic and auditable) rather than just giving a text answer, ensuring the math is always correct.
- **Anonymous (10:22 AM): How do you predict AI will effect jobs in CDM over the next 5 years?**
  - The "Clinical Data Cleaner" role will disappear, replaced by the **"Clinical Data Auditor"** or **"Data Strategist."** You will stop writing individual queries and start managing AI agents that monitor data streams; your value will shift from _finding_ errors to _designing_ the systems that find them.
- **Dr Arathi Joshi (10:22 AM): How can AI clean data just with help of Raw data and protocol**
  - The AI operates in two steps: first, it uses NLP to "read" the Protocol and extract logic (e.g., "Systolic BP > 180 is an AE"). Second, it writes a script to scan the Raw Data against these rules, flagging discrepancies for human review just like a computerized edit check, but created instantly.
- **Anonymous (10:23 AM): How does AI effect CDM process? And how much human intervention will be reduced?**
  - Routine query generation (typos, date logic) will see a **90% reduction** in human intervention, automated by agents. However, complex safety signal detection and protocol deviation review will remain human-led, with AI serving as a "co-pilot" that surfaces relevant patient profiles for you to judge.
- **Rashida Rampurawala (10:23 AM): What level of data privacy does AI manage? especially in a highly regulated Industry like clinical research**
  - Enterprise AI today operates on **"Zero Data Retention"** policies (e.g., Azure OpenAI, AWS Bedrock), where your data is processed in a private instance and _never_ used to train the public model. We also use "PII Masking" middleware that replaces patient names/IDs with synthetic tokens before the data ever touches the LLM.
- **Anonymous (10:24 AM): How do we deal with data leaks and data protection while using AI effectively**
  - We implement **DLP (Data Loss Prevention)** layers that sit between the user and the AI, automatically blocking prompts containing recognizable PII or confidential IP. Additionally, we disable the "Chat History" training features on all corporate accounts to ensure no leakage into the public model.
- **Anonymous (10:24 AM): The LLM learnt from human creation so far, what will happen in the future if humans don’t invent in future, how the LLM going to learn**
  - This is the "Model Collapse" risk; to prevent it, future models are being trained on "curated" synthetic data (verified by humans) and a permanent "anchor set" of pre-2023 human-authored content. We are finding that AI generates novel scientific hypotheses that humans verify, creating a new loop of human-verified, AI-generated training data.
- **Anonymous (10:24 AM): What training programs or resources are available to help staff adapt their skills to work alongside AI tools effectively to help CDM industry?**
  - Focus on **"Computational Thinking"** and **"Prompt Engineering for Science"** rather than just coding syntax; platforms like Coursera and industry bodies (like SCDM) now offer "AI for Clinical Research" tracks. Staff should learn how to read the _code_ an AI generates (Python/SQL) to validate it, rather than learning to write it from scratch.
- **Kedarinath (10:24 AM): What strategies can help bring AI agents into enterprise workspaces that have stringent in-house security policies restricting their use?**
  - Use a **"Bring Your Own Key" (BYOK)** architecture or deploy open-source models (like Llama 3) on your own secure, on-premise servers ("Air-gapped AI"). This ensures the AI brain comes to your data, rather than your data leaving your firewall to go to the AI.
- **Dr Syed (10:26 AM): What impact are large tech players such as Microsoft, Nvidia and Google having on the future of clinical trial design and execution?**
  - They are becoming the "utilities" of pharma: Microsoft (Nuance) is automating physician notes into EDC data, Nvidia is powering the "Digital Twin" simulations for trial arms, and Google (DeepMind) is revolutionizing the biology of drug discovery. They are building the infrastructure rail that all our clinical apps will soon run on.
- **Rashida Rampurawala (10:26 AM): Considering the diversity of data... what are the factors must be considered when Selecting a AI solution**
  - Prioritize **Traceability** (can it show you the source of its answer?) and **Interoperability** (can it ingest messy CSVs, PDFs, and Wearable data?). Avoid "Black Box" solutions; require tools that output verifiable code or citations so you can defend the results to an auditor.
- **Supriya (10:27 AM): What is the correctness or accuracy of data that is being summarized using gen AI... would it still need a human intervention?**
  - Benchmarks in late 2025 show top models (like GPT-4o or Claude 3.5 Sonnet) achieve ~95-99% accuracy on summarization, but the error rate is never zero. Therefore, human intervention is mandatory, shifting from "writing the summary" to **"reviewing the citations"**—you must verify the specific source links the AI provides for every claim.
- **Anonymous (10:28 AM): How do we deal with the hallucinations of the ai models?**
  - We use **Retrieval-Augmented Generation (RAG)** to force the AI to answer _only_ using your uploaded documents, prohibiting it from using its internal "creative" training data. Additionally, for data tasks, we ask the AI to write **Python code** to calculate answers; code either runs correctly or errors out, preventing subtle numerical hallucinations.
- **Dr Arathi Joshi (10:28 AM): What about data security and blinding of studies, using AI can unblind our data**
  - AI is excellent at pattern recognition and could deduce treatment arms from Adverse Event clusters (e.g., spotting a specific side effect). To prevent this, we must **sanitize the input data** before it reaches the AI, removing or masking columns like "Kit ID" or "Visit Date" deviations that could mathematically reveal the blind.
- **Deepak Nema (10:30 AM): Which AI model was used to ingest documents and validate it with ICF...**
  - For checking Protocol vs. ICF, models with massive context windows are best; **Claude 3.5 Sonnet** or **Gemini 1.5 Pro** are currently favored because they can read hundreds of pages at once without "forgetting" details. Specialized tools like **InformGen** are also emerging specifically for this, offering near 100% compliance checking against FDA rules.
- **Mangalam M (10:30 AM): What could a ‘second-opinion AI’ in clinical trials look like...?**
  - It looks like **"Adversarial AI"**: One AI agent (the "Writer") drafts a protocol, and a second, separate AI agent (the "Critic") is prompted to act as an FDA auditor to aggressively find flaws in that draft. This "AI-on-AI" debate catches errors that a single pass might miss, significantly improving quality before a human ever sees it.
- **Anonymous (10:32 AM): Please explain the effective validation process to validate agentic model...**
  - You cannot validate the _agent_ (which is non-deterministic); you must validate the **Agent's Tools** (the Python scripts it calls) and its **Guardrails** (the rules that stop it). Validation focuses on "Range Testing"—running the agent 1,000 times on synthetic data to ensure its failure rate is within an acceptable statistical margin (e.g., <1%).
- **Anonymous (10:33 AM): What are the GenAI topics where industry and academia should focus and research?**
  - The biggest gap is **"Synthetic Patient Arms"**—proving mathematically that AI-simulated patients can replace placebo groups in rare disease trials. Academia needs to solve the "Model Collapse" theory (what happens when AI learns from AI data), while industry focuses on the practical engineering of **Multi-Agent Orchestration**.
- **Anonymous (10:34 AM): What should the learning journey for a CDM professional in AI look like?**
  - Start with **"AI Literacy"** (understanding how LLMs predict tokens), then move to **"Prompt Engineering"** (learning to structure instructions), and finally **"AI Auditing"** (learning to verify code/logic). You do _not_ need to become a software engineer; you need to become an expert _reviewer_ of software outputs.
- **Anonymous (10:38 AM): How many CROs or organizations would be willing to adapt to gen AI... as there is confidentiality involved**
  - Adoption is high (over 50% of large orgs have pilots), but the model is shifting to **"Bring Your Own Key" (BYOK)** where the Pharma company keeps the data in their own secure cloud (Azure/AWS) and simply rents the AI "brain" to process it. No data leaves the sponsor's firewall, which satisfies the confidentiality concerns of most CROs.
- **Anonymous (10:40 AM): How to learn about these models testing as a data management professional?**
  - Focus on **"Adversarial Testing"**—learning how to intentionally try to break the model. Platforms like **Hugging Face** offer free courses on "Red Teaming," and Google Cloud’s "Responsible AI" learning path is excellent for understanding how to test for bias and safety.
- **Anonymous (10:44 AM): If different prompts lead to different outcomes, can we consider the results dependable?**
  - In regulated workflows, we solve this by "locking" the prompt: we engineer a proven prompt once, validate its output, and then hard-code it into an application (like an **AI Agent**) so users just click a button. This turns a variable conversation into a **reproducible software process**.
- **Anonymous (10:44 AM): LLM doing the investigation on lab results seems fine; however, should this be done by Data Managers? Do you expect this to be a new norm in the near future?**
  - Yes, this is the "New Norm." The Data Manager's role is evolving from **"Data Cleaner"** (fixing typos) to **"Data Investigator"** (finding patterns); you will use AI to do the grunt work of scanning the data so you have the time to investigate the complex anomalies it finds.
- **Anonymous (10:46 AM): Is the use of AI justified given the environmental impact of electricity and water consumption in data centers?**
  - It is a serious trade-off, but the industry is mitigating it by moving to **"Small Language Models" (SLMs)** which use 1% of the energy of a GPT-4, and practicing **"Carbon-Aware Computing"**—training models only in regions (like the Nordics) powered by renewable hydro/geothermal energy.
- **Kirubavathi (10:46 AM): So we all believe program/coding language is always accurate, have you ever experienced any programming language made a mistake?**
  - Computers never make **Syntax Errors** (grammar mistakes), but they frequently make **Logic Errors** (doing the wrong thing perfectly). If you ask AI to "calculate the average" but give it the wrong column, it will calculate the wrong answer with 100% accuracy—that is why the human must audit the _logic_, not the math.
- **Naresh (10:47 AM): How do we systematically detect errors related to date, time, or visit mismatch for same subject when comparing external lab file with data captured in the EDC?**
  - Instead of writing manual SQL, we now ask an **AI Agent** to: "Ingest the Lab CSV and EDC CSV, map the 'Visit Date' columns, and output a list of IDs where the difference is >0 days." The AI writes the Python script to do this comparison instantly and reproducibly.
- **Seema Singh (10:47 AM): How can we ensure consistency (“uniformity”) in asking relevant questions so that AI use in clinical trials is effective?**
  - We stop using "Chat" and start using **"Prompt Libraries."** Organizations now bake validated, freeze-framed prompts into their software (e.g., a "Check Protocol" button) so that every Data Manager runs the exact same instruction set every time, ensuring uniformity.
- **Seema Singh (10:47 AM): How reliable is GPT output when dealing with sensitive data like clinical protocols?**
  - Latest benchmarks (late 2025) show ~85-90% accuracy for complex medical reasoning, which is high but not perfect. Therefore, we never use it for **"Final Approval"**—we use it for **"First Drafts"** and **"Gap Analysis,"** always keeping a qualified human as the final signatory.
- **Rashida Rampurawala (10:51 AM): Are we compartmentalising which tasks/activities need AI solutions, as some activities are reliable/faster when done by a human...**
  - Yes, we use a **"Risk-Based Framework."** High-volume, low-risk tasks (like standard query generation) are assigned to AI Agents; low-volume, high-risk tasks (like SAE adjudication or Final Database Lock) are strictly compartmentalized for Human Review.
- **Anonymous (10:53 AM): How can confidentiality be maintained at CROs when they work with multiple sponsors?**
  - We use **"Multi-Tenant Architecture"** with **"Bring Your Own Key" (BYOK).** This means the AI model is shared, but every Sponsor's data lives in a chemically isolated database encrypted with a key that only the Sponsor (not the CRO) owns.
- **Srinivasulu Elluri (10:56 AM): Will the integration of domain specific knowledge health using ontology and taxonomy increase the accuracy?**
  - Absolutely. "Grounding" an LLM in a rigid ontology (like **CDISC** or **SNOMED-CT**) is the best way to stop hallucinations; it forces the AI to pick from a pre-defined list of valid terms rather than making up its own.
- **Sanskar (11:01 AM): AI is reducing thinking capacity of human. Is it true and how to avoid?**
  - It reduces **"Rote Thinking"** (memorizing codes) but demands higher **"Critical Thinking."** To avoid atrophy, treat AI as a "Junior Partner"—always ask it _why_ it reached a conclusion and force yourself to verify its reasoning before accepting the output.
- **Anonymous (11:02 AM): Will AI change the education system in future? The education system will now focus on AI knowledge rather than focussing on languages like Java, Dotnet etc.**
  - Education will shift from **"Syntax"** (how to write code) to **"Semantics"** (how to structure a problem). Knowing _Java_ will be less important than knowing _Clinical Trial Logic_, because the AI can write the Java if you can clearly explain the clinical rule.
